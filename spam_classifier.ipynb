{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.匯入 Dataset (panda對文字資料好用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath='C:\\\\Users\\\\zxc98\\\\spam_classifier_test/spamraw.csv'\n",
    "def readData_rawSMS(filepath):\n",
    "    #只需要columns 0，1\n",
    "    data_rawSMS   = pd.read_csv(filepath,usecols=[0,1],encoding='latin-1')\n",
    "    data_rawSMS.columns=['label','content']\n",
    "    return   data_rawSMS\n",
    "data_rawSMS = readData_rawSMS(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or Â£10,000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Hope you are having a good week. Just checking in\n",
       "1   ham                            K..give back my thanks.\n",
       "2   ham        Am also doing in cbe only. But have to pay.\n",
       "3  spam  complimentary 4 STAR Ibiza Holiday or Â£10,000...\n",
       "4  spam  okmail: Dear Dave this is your final notice to..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rawSMS.iloc[8].content\n",
    "data_rawSMS.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.這邊切割數據集單純用亂數取(0~1之間數字)\n",
    "## >0.5   Training Data\n",
    "## <=0.5 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>Marvel Mobile Play the official Ultimate Spide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>fyi I'm at usf now, swing by the room whenever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Hope you are having a good week. Just checking in\n",
       "2   ham        Am also doing in cbe only. But have to pay.\n",
       "4  spam  okmail: Dear Dave this is your final notice to...\n",
       "8  spam  Marvel Mobile Play the official Ultimate Spide...\n",
       "9   ham     fyi I'm at usf now, swing by the room whenever"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "def Separate_TrainAndTest(data_rawSMS):\n",
    "    n=int(data_rawSMS.shape[0])  #shape輸出為(行，列)\n",
    "    tmp_train=(np.random.rand(n)>0.5)\n",
    "    return data_rawSMS.iloc[np.where(tmp_train==True)[0]], data_rawSMS.iloc[np.where(tmp_train==False)[0]]\n",
    "data_rawtrain,data_rawtest=Separate_TrainAndTest(data_rawSMS)\n",
    "data_rawtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 計算HAM和SPAM 的 TF-IDF差異 DIFF\n",
    "## 值越大的代表他在SPAM的可能性更大\n",
    "「size_table」: 要選多少個重要的「詞」出來，等於決定特徵向量的維度數。Default:我設成200。\n",
    " 「ignore」: 英文字，字少於幾個以下就不要算，比如: 「I」就是1個字，「no」是2個字。Default:我設成3個。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_key_list(data_rawtrain, size_table=200,ignore=3):\n",
    "    dict_spam_raw = dict()\n",
    "    dict_genuine_raw = dict()\n",
    "    dict_IDF = dict()\n",
    "\n",
    "    # 去除字母外的所有內容.\n",
    "    for i in range(data_rawSMS.shape[0]):\n",
    "        finds = re.findall('[A-Za-z]+', data_rawSMS.iloc[i].content)\n",
    "        if data_rawSMS.iloc[i].label == 'spam':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower() #英文轉成小寫\n",
    "                try:\n",
    "                    dict_spam_raw[find] = dict_spam_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,1) #若是新的文字因為後面get找不到這個索引會返回逗點後面的值\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower()\n",
    "                try:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,1)\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,0)\n",
    "\n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if len(find)<ignore: continue\n",
    "            find = find.lower()\n",
    "            if not(find in word_set):\n",
    "                try:\n",
    "                    dict_IDF[find] = dict_IDF[find] + 1\n",
    "                except:\t\n",
    "                    dict_IDF[find] = dict_IDF.get(find,1)\n",
    "            word_set.add(find)\n",
    "    word_df = pd.DataFrame(list(zip(dict_genuine_raw.keys(),dict_genuine_raw.values(),dict_spam_raw.values(),dict_IDF.values())))\n",
    "    word_df.columns = ['keyword','genuine','spam','IDF']\n",
    "    word_df['genuine'] = word_df['genuine'].astype('float')/data_rawtrain[data_rawtrain['label']=='genuine'].shape[0]\n",
    "    word_df['spam'] = word_df['spam'].astype('float')/data_rawtrain[data_rawtrain['label']=='spam'].shape[0]\n",
    "    word_df['IDF'] = np.log10(word_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['genuine_IDF'] = word_df['genuine']*word_df['IDF']\n",
    "    word_df['spam_IDF'] = word_df['spam']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_IDF']-word_df['genuine_IDF']\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)  \n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for word in selected_spam_key.head(size_table).keyword:\n",
    "        keyword_dict.update({word.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict   \n",
    "# build a tabu list based on the training data\n",
    "size_table = 200                 # 多少特徵維度去分類SPAM\n",
    "word_len_ignored = 3            # 忽略那些比這個還要小的字詞\n",
    "keyword_dict=generate_key_list(data_rawtrain, size_table, word_len_ignored)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope:0\n",
      "hannaford:1\n",
      "durham:2\n",
      "cheyyamo:3\n",
      "jot:4\n",
      "adds:5\n",
      "stressfull:6\n",
      "forgive:7\n",
      "tick:8\n",
      "dehydrated:9\n",
      "chex:10\n",
      "wheat:11\n",
      "jam:12\n",
      "tosend:13\n",
      "bags:14\n",
      "garbage:15\n",
      "tomorro:16\n",
      "firsg:17\n",
      "burnt:18\n",
      "keys:19\n",
      "promptly:20\n",
      "honestly:21\n",
      "western:22\n",
      "dub:23\n",
      "reserved:24\n",
      "haiz:25\n",
      "accept:26\n",
      "messaged:27\n",
      "designation:28\n",
      "yummy:29\n",
      "answr:30\n",
      "xam:31\n",
      "students:32\n",
      "drinkin:33\n",
      "getiing:34\n",
      "reffering:35\n",
      "fake:36\n",
      "tightly:37\n",
      "netflix:38\n",
      "dodgey:39\n",
      "hiphop:40\n",
      "scared:41\n",
      "kaiez:42\n",
      "inever:43\n",
      "finalise:44\n",
      "prepared:45\n",
      "leftovers:46\n",
      "plate:47\n",
      "prayers:48\n",
      "greeting:49\n",
      "somewheresomeone:50\n",
      "regret:51\n",
      "taste:52\n",
      "convincing:53\n",
      "deposit:54\n",
      "pax:55\n",
      "farm:56\n",
      "girlie:57\n",
      "concentrating:58\n",
      "tiring:59\n",
      "haiyoh:60\n",
      "reminder:61\n",
      "fish:62\n",
      "beatings:63\n",
      "daywith:64\n",
      "device:65\n",
      "chastity:66\n",
      "brum:67\n",
      "brilliantly:68\n",
      "skyving:69\n",
      "mei:70\n",
      "referin:71\n",
      "symbol:72\n",
      "issues:73\n",
      "dabooks:74\n",
      "neglect:75\n",
      "lifted:76\n",
      "hopes:77\n",
      "approaches:78\n",
      "lifeis:79\n",
      "joys:80\n",
      "logged:81\n",
      "myspace:82\n",
      "freaking:83\n",
      "joanna:84\n",
      "stayed:85\n",
      "knees:86\n",
      "grazed:87\n",
      "splat:88\n",
      "fell:89\n",
      "toaday:90\n",
      "hooch:91\n",
      "respond:92\n",
      "himso:93\n",
      "fed:94\n",
      "arguments:95\n",
      "thin:96\n",
      "nimbomsons:97\n",
      "entrepreneurs:98\n",
      "anyways:99\n",
      "developer:100\n",
      "format:101\n",
      "challenge:102\n",
      "tlk:103\n",
      "repeat:104\n",
      "hopefully:105\n",
      "lips:106\n",
      "biggest:107\n",
      "courtroom:108\n",
      "division:109\n",
      "twiggs:110\n",
      "affidavit:111\n",
      "bookshelf:112\n",
      "dismissial:113\n",
      "visiting:114\n",
      "atten:115\n",
      "jide:116\n",
      "conducts:117\n",
      "teaches:118\n",
      "teacher:119\n",
      "strict:120\n",
      "prepare:121\n",
      "wipe:122\n",
      "reltnship:123\n",
      "dialogue:124\n",
      "tonght:125\n",
      "poking:126\n",
      "gimmi:127\n",
      "goss:128\n",
      "fetching:129\n",
      "ente:130\n",
      "ctla:131\n",
      "drove:132\n",
      "cudnt:133\n",
      "denying:134\n",
      "piss:135\n",
      "allday:136\n",
      "wanting:137\n",
      "penis:138\n",
      "guoyang:139\n",
      "tirunelvai:140\n",
      "evenings:141\n",
      "brah:142\n",
      "suddenly:143\n",
      "sweatter:144\n",
      "pooja:145\n",
      "scrumptious:146\n",
      "initiate:147\n",
      "hole:148\n",
      "philosophical:149\n",
      "vijaykanth:150\n",
      "popped:151\n",
      "immed:152\n",
      "aah:153\n",
      "phyhcmk:154\n",
      "nus:155\n",
      "staff:156\n",
      "challenging:157\n",
      "bills:158\n",
      "smoothly:159\n",
      "noooooooo:160\n",
      "chip:161\n",
      "conform:162\n",
      "mca:163\n",
      "thout:164\n",
      "summon:165\n",
      "slaaaaave:166\n",
      "evr:167\n",
      "evn:168\n",
      "irritating:169\n",
      "attractive:170\n",
      "shy:171\n",
      "attitude:172\n",
      "rowdy:173\n",
      "sentiment:174\n",
      "naughty:175\n",
      "edu:176\n",
      "mus:177\n",
      "pull:178\n",
      "comfort:179\n",
      "engin:180\n",
      "bhaskar:181\n",
      "sos:182\n",
      "friendships:183\n",
      "significance:184\n",
      "morro:185\n",
      "puttin:186\n",
      "luvd:187\n",
      "frndshp:188\n",
      "squeeeeeze:189\n",
      "recharged:190\n",
      "success:191\n",
      "wasting:192\n",
      "tallent:193\n",
      "combination:194\n",
      "payback:195\n",
      "woould:196\n",
      "chickened:197\n",
      "goods:198\n",
      "shesil:199\n"
     ]
    }
   ],
   "source": [
    "#排序越前面的代表出現在SPAM的機率越大\n",
    "for key,value in keyword_dict.items():\n",
    "    print('{key}:{value}'.format(key = key, value = value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.將Train 和 Test資料轉換為特徵向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Content(content, keyword_dict):\n",
    "    #判斷是否有此特徵\n",
    "    m = len(keyword_dict) #維度數量\n",
    "    res = np.int_(np.zeros(m)) #建置一個幾維度的向量\n",
    "    finds = re.findall('[A-Za-z]+', content) #將文本內容切割(類似中文斷詞)\n",
    "    for find in finds:\n",
    "        find=find.lower() #改小寫\n",
    "        try:\n",
    "            #若比對完有此特徵則特徵改為1\n",
    "            i = keyword_dict[find] \n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res\n",
    "def raw2feature(data_rawtrain,data_rawtest,keyword_dict):\n",
    "    n_train = data_rawtrain.shape[0]\n",
    "    n_test = data_rawtest.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "    X_train = np.zeros((n_train,m));\n",
    "    X_test = np.zeros((n_test,m));\n",
    "    Y_train = np.int_(data_rawtrain.label=='spam')\n",
    "    Y_test = np.int_(data_rawtest.label=='spam')\n",
    "    for i in range(n_train):\n",
    "        X_train[i,:] = convert_Content(data_rawtrain.iloc[i].content, keyword_dict)\n",
    "    for i in range(n_test):\n",
    "        X_test[i,:] = convert_Content(data_rawtest.iloc[i].content, keyword_dict)\n",
    "        \n",
    "    return [X_train,Y_train],[X_test,Y_test]\n",
    "     \n",
    "Train,Test=raw2feature(data_rawtrain,data_rawtest,keyword_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.建置分類器NB和RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuarcy NBclassifier : 86.78％\n",
      "Training Accuarcy RF: 87.41％\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB       \n",
    "def learn(Train):\n",
    "    model_NB = BernoulliNB()\n",
    "    model_NB.fit(Train[0], Train[1])\n",
    "    Y_hat_NB = model_NB.predict(Train[0])\n",
    "\n",
    "    model_RF = RandomForestClassifier(n_estimators=10, max_depth=None,\\\n",
    "                                 min_samples_split=2, random_state=0)\n",
    "    model_RF.fit(Train[0], Train[1])\n",
    "    Y_hat_RF = model_RF.predict(Train[0])\n",
    "    \n",
    "    n=np.size(Train[1])\n",
    "    print('Training Accuarcy NBclassifier : {:.2f}％'.format(sum(np.int_(Y_hat_NB==Train[1]))*100./n))\n",
    "    print('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==Train[1]))*100./n))\n",
    "    return model_NB,model_RF\n",
    "# train the Random Forest and the Naive Bayes Model using training data\n",
    "model_NB,model_RF=learn(Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.分類器NB和RF預測test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuarcy: 84.95％ (sklearn.naive_bayes)\n",
      "Testing Accuarcy: 85.79％ (sklearn.ensemble.forest)\n",
      "HAM: Call back for anytime.\n",
      "HAM: All goods will always be special until October 20\n"
     ]
    }
   ],
   "source": [
    "def test(Test,model):\n",
    "    Y_hat = model.predict(Test[0])\n",
    "    n=np.size(Test[1])\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==Test[1]))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test(Test,model_NB)\n",
    "test(Test,model_RF)\n",
    "#######\n",
    "def predictSMS(SMS,model,keyword_dict):\n",
    "    X = convert_Content(SMS, keyword_dict)\n",
    "    Y_hat = model.predict(X.reshape(1,-1))\n",
    "    if int(Y_hat) == 1:\n",
    "        print ('SPAM: {}'.format(SMS))\n",
    "    else:\n",
    "        print ('HAM: {}'.format(SMS))    \n",
    "\n",
    "inputstr=('Call back for anytime.')\n",
    "predictSMS(inputstr,model_NB,keyword_dict)\n",
    "\n",
    "inputstr=('All goods will always be special until October 20')\n",
    "predictSMS(inputstr,model_NB,keyword_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
