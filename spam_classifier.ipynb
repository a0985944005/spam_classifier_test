{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.匯入 Dataset (panda對文字資料好用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath='C:\\\\Users\\\\zxc98\\\\WordCloud/spam_1.csv'\n",
    "def readData_rawSMS(filepath):\n",
    "    #只需要columns 0，1\n",
    "    data_rawSMS   = pd.read_csv(filepath,usecols=[0,1],encoding='latin-1')\n",
    "    data_rawSMS.columns=['label','content']\n",
    "    return   data_rawSMS\n",
    "data_rawSMS = readData_rawSMS(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rawSMS.iloc[8].content\n",
    "#data_rawSMS.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.這邊切割數據集單純用亂數取(0~1之間數字)\n",
    "## >0.5   Training Data\n",
    "## <=0.5 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "def Separate_TrainAndTest(data_rawSMS):\n",
    "    n=int(data_rawSMS.shape[0])  #shape輸出為(行，列)\n",
    "    tmp_train=(np.random.rand(n)>0.5)\n",
    "    return data_rawSMS.iloc[np.where(tmp_train==True)[0]], data_rawSMS.iloc[np.where(tmp_train==False)[0]]\n",
    "data_rawtrain,data_rawtest=Separate_TrainAndTest(data_rawSMS)\n",
    "data_rawtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 計算HAM和SPAM 的 TF-IDF差異 DIFF\n",
    "## 值越大的代表他在SPAM的可能性更大\n",
    "「size_table」: 要選多少個重要的「詞」出來，等於決定特徵向量的維度數。Default:我設成200。\n",
    " 「ignore」: 英文字，字少於幾個以下就不要算，比如: 「I」就是1個字，「no」是2個字。Default:我設成3個。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_key_list(data_rawtrain, size_table=200,ignore=3):\n",
    "    dict_spam_raw = dict()\n",
    "    dict_genuine_raw = dict()\n",
    "    dict_IDF = dict()\n",
    "\n",
    "    # 去除字母外的所有內容.\n",
    "    for i in range(data_rawSMS.shape[0]):\n",
    "        finds = re.findall('[A-Za-z]+', data_rawSMS.iloc[i].content)\n",
    "        if data_rawSMS.iloc[i].label == 'spam':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower() #英文轉成小寫\n",
    "                try:\n",
    "                    dict_spam_raw[find] = dict_spam_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,1) #若是新的文字因為後面get找不到這個索引會返回逗點後面的值\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                find = find.lower()\n",
    "                try:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,1)\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,0)\n",
    "\n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if len(find)<ignore: continue\n",
    "            find = find.lower()\n",
    "            if not(find in word_set):\n",
    "                try:\n",
    "                    dict_IDF[find] = dict_IDF[find] + 1\n",
    "                except:\t\n",
    "                    dict_IDF[find] = dict_IDF.get(find,1)\n",
    "            word_set.add(find)\n",
    "    word_df = pd.DataFrame(list(zip(dict_genuine_raw.keys(),dict_genuine_raw.values(),dict_spam_raw.values(),dict_IDF.values())))\n",
    "    word_df.columns = ['keyword','genuine','spam','IDF']\n",
    "    word_df['genuine'] = word_df['genuine'].astype('float')/data_rawtrain[data_rawtrain['label']=='genuine'].shape[0]\n",
    "    word_df['spam'] = word_df['spam'].astype('float')/data_rawtrain[data_rawtrain['label']=='spam'].shape[0]\n",
    "    word_df['IDF'] = np.log10(word_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['genuine_IDF'] = word_df['genuine']*word_df['IDF']\n",
    "    word_df['spam_IDF'] = word_df['spam']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_IDF']-word_df['genuine_IDF']\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)  \n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for word in selected_spam_key.head(size_table).keyword:\n",
    "        keyword_dict.update({word.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict   \n",
    "# build a tabu list based on the training data\n",
    "size_table = 200                 # 多少特徵維度去分類SPAM\n",
    "word_len_ignored = 3            # 忽略那些比這個還要小的字詞\n",
    "keyword_dict=generate_key_list(data_rawtrain, size_table, word_len_ignored)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "until:0\n",
      "benefits:1\n",
      "concern:2\n",
      "fuelled:3\n",
      "incident:4\n",
      "shoranur:5\n",
      "fated:6\n",
      "belongs:7\n",
      "programs:8\n",
      "accumulation:9\n",
      "predicting:10\n",
      "amount:11\n",
      "risks:12\n",
      "amazing:13\n",
      "deliver:14\n",
      "relatives:15\n",
      "plenty:16\n",
      "residency:17\n",
      "permanent:18\n",
      "registration:19\n",
      "begun:20\n",
      "medicine:21\n",
      "mmmmm:22\n",
      "computers:23\n",
      "prior:24\n",
      "grief:25\n",
      "violence:26\n",
      "environment:27\n",
      "behind:28\n",
      "flute:29\n",
      "dabbles:30\n",
      "newscaster:31\n",
      "rolled:32\n",
      "dock:33\n",
      "korli:34\n",
      "achieve:35\n",
      "tui:36\n",
      "motive:37\n",
      "tor:38\n",
      "ultimately:39\n",
      "superior:40\n",
      "pressies:41\n",
      "goods:42\n",
      "someplace:43\n",
      "advice:44\n",
      "burnt:45\n",
      "promptly:46\n",
      "honestly:47\n",
      "terrific:48\n",
      "misplaced:49\n",
      "ktv:50\n",
      "splendid:51\n",
      "released:52\n",
      "necessity:53\n",
      "ego:54\n",
      "strange:55\n",
      "struggling:56\n",
      "drama:57\n",
      "excused:58\n",
      "greetings:59\n",
      "papers:60\n",
      "shoving:61\n",
      "appointments:62\n",
      "drizzling:63\n",
      "srs:64\n",
      "kanji:65\n",
      "jungle:66\n",
      "pansy:67\n",
      "barcelona:68\n",
      "easiest:69\n",
      "jenne:70\n",
      "key:71\n",
      "locks:72\n",
      "lock:73\n",
      "reppurcussions:74\n",
      "cosign:75\n",
      "bognor:76\n",
      "hcl:77\n",
      "statements:78\n",
      "pleasant:79\n",
      "props:80\n",
      "citylink:81\n",
      "yunny:82\n",
      "arrival:83\n",
      "maga:84\n",
      "whr:85\n",
      "happens:86\n",
      "afford:87\n",
      "fwiw:88\n",
      "reliant:89\n",
      "bids:90\n",
      "cars:91\n",
      "resume:92\n",
      "dai:93\n",
      "indyarocks:94\n",
      "telephonic:95\n",
      "suman:96\n",
      "freshers:97\n",
      "requires:98\n",
      "wheel:99\n",
      "vid:100\n",
      "combination:101\n",
      "kochi:102\n",
      "usc:103\n",
      "olowoyey:104\n",
      "technologies:105\n",
      "pandy:106\n",
      "arts:107\n",
      "gobi:108\n",
      "smear:109\n",
      "pap:110\n",
      "checkup:111\n",
      "cancer:112\n",
      "bang:113\n",
      "ruin:114\n",
      "battle:115\n",
      "rcb:116\n",
      "tiwary:117\n",
      "prasad:118\n",
      "sundayish:119\n",
      "coupla:120\n",
      "fab:121\n",
      "digi:122\n",
      "treated:123\n",
      "forfeit:124\n",
      "edu:125\n",
      "argentina:126\n",
      "secretary:127\n",
      "taxt:128\n",
      "nalla:129\n",
      "response:130\n",
      "sensible:131\n",
      "impressively:132\n",
      "bros:133\n",
      "amongst:134\n",
      "bro:135\n",
      "remains:136\n",
      "eruku:137\n",
      "naal:138\n",
      "scarcasim:139\n",
      "lil:140\n",
      "booty:141\n",
      "shaking:142\n",
      "spending:143\n",
      "pull:144\n",
      "dollar:145\n",
      "lool:146\n",
      "pos:147\n",
      "tie:148\n",
      "massage:149\n",
      "dramastorm:150\n",
      "particular:151\n",
      "lacking:152\n",
      "gosh:153\n",
      "ibm:154\n",
      "death:155\n",
      "apes:156\n",
      "timin:157\n",
      "grams:158\n",
      "upping:159\n",
      "shrink:160\n",
      "cysts:161\n",
      "ovarian:162\n",
      "developed:163\n",
      "flow:164\n",
      "instructions:165\n",
      "conform:166\n",
      "ours:167\n",
      "department:168\n",
      "issues:169\n",
      "aspects:170\n",
      "safety:171\n",
      "elaborating:172\n",
      "october:173\n",
      "erm:174\n",
      "toshiba:175\n",
      "spose:176\n",
      "unni:177\n",
      "rimac:178\n",
      "above:179\n",
      "reality:180\n",
      "hamper:181\n",
      "breakfast:182\n",
      "hon:183\n",
      "marrow:184\n",
      "challenging:185\n",
      "bills:186\n",
      "hoped:187\n",
      "smoothly:188\n",
      "mental:189\n",
      "innocent:190\n",
      "restocked:191\n",
      "starring:192\n",
      "luckily:193\n",
      "eachother:194\n",
      "dabooks:195\n",
      "wrk:196\n",
      "dosomething:197\n",
      "squid:198\n",
      "arestaurant:199\n"
     ]
    }
   ],
   "source": [
    "#排序越前面的代表出現在SPAM的機率越大\n",
    "for key,value in keyword_dict.items():\n",
    "    print('{key}:{value}'.format(key = key, value = value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.將Train 和 Test資料轉換為特徵向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([0, 0, 1, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_Content(content, keyword_dict):\n",
    "    #判斷是否有此特徵\n",
    "    m = len(keyword_dict) #維度數量\n",
    "    res = np.int_(np.zeros(m)) #建置一個幾維度的向量\n",
    "    finds = re.findall('[A-Za-z]+', content) #將文本內容切割(類似中文斷詞)\n",
    "    for find in finds:\n",
    "        find=find.lower() #改小寫\n",
    "        try:\n",
    "            #若比對完有此特徵則特徵改為1\n",
    "            i = keyword_dict[find] \n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res\n",
    "def raw2feature(data_rawtrain,data_rawtest,keyword_dict):\n",
    "    n_train = data_rawtrain.shape[0]\n",
    "    n_test = data_rawtest.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "    X_train = np.zeros((n_train,m));\n",
    "    X_test = np.zeros((n_test,m));\n",
    "    Y_train = np.int_(data_rawtrain.label=='spam')\n",
    "    Y_test = np.int_(data_rawtest.label=='spam')\n",
    "    for i in range(n_train):\n",
    "        X_train[i,:] = convert_Content(data_rawtrain.iloc[i].content, keyword_dict)\n",
    "    for i in range(n_test):\n",
    "        X_test[i,:] = convert_Content(data_rawtest.iloc[i].content, keyword_dict)\n",
    "        \n",
    "    return [X_train,Y_train],[X_test,Y_test]\n",
    "     \n",
    "Train,Test=raw2feature(data_rawtrain,data_rawtest,keyword_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.建置分類器NB和RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuarcy NBclassifier : 85.58％\n",
      "Training Accuarcy RF: 86.54％\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB       \n",
    "def learn(Train):\n",
    "    model_NB = BernoulliNB()\n",
    "    model_NB.fit(Train[0], Train[1])\n",
    "    Y_hat_NB = model_NB.predict(Train[0])\n",
    "\n",
    "    model_RF = RandomForestClassifier(n_estimators=10, max_depth=None,\\\n",
    "                                 min_samples_split=2, random_state=0)\n",
    "    model_RF.fit(Train[0], Train[1])\n",
    "    Y_hat_RF = model_RF.predict(Train[0])\n",
    "    \n",
    "    n=np.size(Train[1])\n",
    "    print('Training Accuarcy NBclassifier : {:.2f}％'.format(sum(np.int_(Y_hat_NB==Train[1]))*100./n))\n",
    "    print('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==Train[1]))*100./n))\n",
    "    return model_NB,model_RF\n",
    "# train the Random Forest and the Naive Bayes Model using training data\n",
    "model_NB,model_RF=learn(Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.分類器NB和RF預測test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuarcy: 85.85％ (sklearn.naive_bayes)\n",
      "Testing Accuarcy: 86.73％ (sklearn.ensemble.forest)\n",
      "HAM: Call back for anytime.\n",
      "SPAM: All goods will always be special until October 20\n"
     ]
    }
   ],
   "source": [
    "def test(Test,model):\n",
    "    Y_hat = model.predict(Test[0])\n",
    "    n=np.size(Test[1])\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==Test[1]))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test(Test,model_NB)\n",
    "test(Test,model_RF)\n",
    "#######\n",
    "def predictSMS(SMS,model,keyword_dict):\n",
    "    X = convert_Content(SMS, keyword_dict)\n",
    "    Y_hat = model.predict(X.reshape(1,-1))\n",
    "    if int(Y_hat) == 1:\n",
    "        print ('SPAM: {}'.format(SMS))\n",
    "    else:\n",
    "        print ('HAM: {}'.format(SMS))    \n",
    "\n",
    "inputstr=('Call back for anytime.')\n",
    "predictSMS(inputstr,model_NB,keyword_dict)\n",
    "\n",
    "inputstr=('All goods will always be special until October 20')\n",
    "predictSMS(inputstr,model_NB,keyword_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
